<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, minimum-scale=1.0">
    <title>EU AI Act Code of Practice</title>
    <link rel="stylesheet" href="https://edwardtufte.github.io/tufte-css/tufte.css" />
    <link rel="stylesheet" href="styles.css" />
</head>

<body>
    <article role="main">
        <!-- Navigation toggle -->
        <button class="nav-toggle" aria-expanded="false" aria-controls="side-nav" aria-label="Toggle navigation menu">
            <span class="nav-icon" aria-hidden="true">&#9776;</span>
            <span class="nav-label">Table of Contents</span>
        </button>

        <!-- Navigation -->
        <nav id="side-nav" class="side-nav" role="navigation" aria-label="Table of contents">
            <div class="nav-controls">
                <div class="theme-control">
                    <label for="theme-select">Theme:</label>
                    <select id="theme-select" aria-label="Select color theme">
                        <option value="system">üñ•Ô∏è System Default</option>
                        <option value="light">‚òÄÔ∏è Light</option>
                        <option value="dark">üåô Dark</option>
                    </select>
                </div>
            </div>
            <div class="keyboard-shortcuts" aria-label="Keyboard shortcuts">
                <h4>Keyboard Shortcuts</h4>
                <ul>
                    <li><kbd>‚Üë</kbd>/<kbd>‚Üì</kbd> Navigate headlines</li>
                    <li><kbd>‚Üê</kbd>/<kbd>‚Üí</kbd> Navigate commitments</li>
                    <li><kbd>1</kbd>To top<kbd>2</kbd>To glossary</li>
                    <li><kbd>3</kbd>To recitals<kbd>4</kbd>Toggle all recitals</li>
                </ul>
            </div>
            <div id="nav-content" class="nav-content">
            </div>
        </nav>

        <h1>EU AI Act Code of Practice</h1>

        <section class="main-content" aria-label="Main content">
            <p class="subtitle">Second Draft - 19/12/2024</p>

            <div class="disclaimer-box" role="complementary" aria-label="Important disclaimer">
                <h4>IMPORTANT DISCLAIMER & HOW TO CONTRIBUTE</h4>
                <p>This is a free, best-effort service to make the EU AI Act Code of Practice more accessible to
                    stakeholders. While we strive
                    for accuracy, this is NOT an official source.</p>

                <p><strong>Please always refer to the <a
                            href="https://digital-strategy.ec.europa.eu/en/library/second-draft-general-purpose-ai-code-practice-published-written-independent-experts">official
                            PDF document</a> for the authoritative version.</strong> Any discrepancies between this
                    version and the official document should be considered errors on our part.</p>

                <p>Help us improve:</p>
                <ul>
                    <li>Report errors via email: <a
                            href="mailto:alexander.zacherl@googlemail.com">alexander.zacherl@googlemail.com</a></li>
                    <li>Submit a pull request on <a href="https://github.com/yourusername/cop-reader">GitHub</a></li>
                </ul>
            </div>

            <h2>Opening statement by the Chairs and Vice-Chairs</h2>

            <p>As the Chairs and Vice-Chairs of the four Working Groups, we hereby present the second draft of the
                General-Purpose AI Code of Practice under the AI Act (the "Code"). Participants in the Working Groups
                and observers of the Code of Practice Plenary are welcome to submit written feedback on this draft by 15
                January 2025 via a dedicated survey shared with them.</p>

            <p>We would like to emphasise that the second draft remains a work-in-progress. Given the short timeframe
                between receiving feedback on the first draft and publishing this second draft, we have focused
                primarily on providing clarifications, adding essential details, and refining our approach to
                proportionality.</p>

            <p>Incorporation of specific feedback at this stage does not guarantee its inclusion in the final Code ‚Äì we
                will have more time to carefully discuss and evaluate various Commitments and Measures before the third
                draft, and significant updates will likely occur. Conversely, where certain elements remain unchanged in
                this draft, this does not indicate permanence ‚Äì we simply may not have addressed these aspects yet.</p>

            <p>This second draft of the Code addresses key considerations for providers of general-purpose AI models and
                providers of general-purpose AI models with systemic risk when complying with Chapter V of the AI Act,
                through four Working Groups working in close collaboration:</p>
            <ul>
                <li>Working Group 1: Transparency and copyright-related rules</li>
                <li>Working Group 2: Risk assessment for systemic risk</li>
                <li>Working Group 3: Technical risk mitigation for systemic risk</li>
                <li>Working Group 4: Governance risk mitigation for systemic risk</li>
            </ul>

            <p>Working Group 1 Transparency applies to all general-purpose AI models, except for those that are released
                under a free and open-source licence satisfying the conditions specified in Article 53(2) AI Act and not
                classified as general-purpose AI models with systemic risk. Working Group 1 Copyright applies to all
                general-purpose AI models. Working Groups 2, 3, and 4, along with the corresponding Section III only
                apply to providers of general-purpose AI models classified as general-purpose AI models with systemic
                risk based on Article 51 AI Act.</p>

            <p>Following a thorough review of the feedback received by stakeholders on the first draft, we have refined
                Commitments and Measures and expanded the Code's provisions while maintaining its Objectives. We present
                this second draft as a foundation for further development. The next draft will draw on your feedback
                provided via the EU survey, in provider workshops, and in Working Group meetings. Thus far, we have
                found your feedback extremely helpful, resulting in substantial changes. We therefore encourage
                stakeholders to continue providing comprehensive feedback on all aspects of the Code, including both new
                and unchanged elements. Your feedback will help shape the final version of the Code, which will play a
                crucial role in guiding the future of general-purpose AI model development and deployment.</p>

            <p>We have once again included a high-level drafting plan which outlines our guiding principles for the
                Code, and the assumptions it is based on. While we continue to engage in thorough deliberations
                regarding specific Commitments, Measures and Key Performance Indicators (KPIs), we hope the drafting
                plan provides stakeholders clarity on the potential form and content of the final Code.</p>

            <p>Note that the exemplary KPIs included in this version of the Code are preliminary, and subject to review
                and revision. For example, while some are quantitative, others are more qualitative. Thus, we strongly
                encourage, and welcome, feedback on the KPIs.</p>

            <p>The AI Act came into force on 1 August 2024, stating that the final version of the Code should be ready
                by 2 May 2025. The second draft builds upon previous work while aiming to provide a "future-proof" Code,
                appropriate for the next generation of models which will be developed and released in 2025 and
                thereafter.</p>

            <p>In formulating this second draft, we have been principally guided by the provisions in the AI Act as to
                matters within the scope of the Code. Accordingly, unless the context and definition contained within
                the Code indicates otherwise, the terms used in the Code refer to identical terms from the AI Act. We
                have not included exhaustive references to provisions in the AI Act in this second draft but expect to
                do so in future iterations.</p>

            <p>Like the first draft, this document is the result of a collaborative effort involving hundreds of
                participants from across industry, academia, and civil society. It has been informed by feedback
                received in response to the first draft, which has been insightful and instructive in our drafting
                process. We continue to be informed by the evolving literature on AI governance, international
                approaches (as specified in Article 56(1) AI Act), Union law codes of practice (such as the Code of
                Practice on Disinformation), industry best practice, and the expertise and experience of providers and
                Working Group members.</p>

            <h3>Key features of the development process of the Code include:</h3>
            <ul>
                <li>Drafted by Chairs and Vice-Chairs who were selected by the AI Office for their expertise,
                    experience, independence (including absence of financial interests), and to ensure gender and
                    geographic diversity.</li>
                <li>A multi-stakeholder consultation which closed in September and received 427 submissions</li>
                <li>A multi-stakeholder survey on the first draft of the Code which received 354 submissions, with more
                    surveys to come</li>
                <li>Organisational support and legal advice from the AI Office</li>
                <li>Provider workshops led by Chairs and Vice-Chairs</li>
                <li>Four specialised Working Group meetings led by Chairs and Vice-Chairs</li>
                <li>Meetings with representatives from EU Member States in the AI Board and from the European Parliament
                </li>
            </ul>

            <p>Additional time for consultation and deliberation ‚Äì both externally and internally ‚Äì will be needed to
                refine and improve the current draft. As a group of independent Chairs and Vice-Chairs, we strive to
                make this process as transparent and accessible to stakeholders as possible, aiming to share our work
                and our thinking as early as possible, while taking sufficient time to coordinate and discuss key
                questions within Working Groups. We count on your continued engaged collaboration and constructive
                criticism.</p>

            <p>Finally, we wish to highlight that, at this stage in the drafting process, one of our central priorities
                has been to clearly communicate our motivations and reasoning regarding the issues we are addressing.
                Many of these issues are nuanced and complex, and we aim to convey them transparently through the draft
                text of the Code. Further, we used our time mainly to refine Commitments, Measures, and KPIs, rather
                than ensuring all the parts of the Code seamlessly fit together and are easy to understand. We will work
                to improve these aspects in subsequent iterations to strengthen the Code.</p>

            <p>We welcome written feedback by the Code of Practice Plenary participants and observers by 15 January
                2025, via a dedicated survey shared with them.</p>

            <p>Thank you for your support!</p>

            <div class="signatures">
                <div class="signatures-grid">
                    <div class="signature-column">
                        <div class="signature-cell">
                            <div class="name">Nuria Oliver</div>
                            <div class="group">Working Group 1</div>
                            <div class="role">Co-Chair</div>
                        </div>
                        <div class="signature-cell">
                            <div class="name">Rishi Bommasani</div>
                            <div class="group">Working Group 1</div>
                            <div class="role">Vice-Chair</div>
                        </div>
                    </div>
                    <div class="signature-column">
                        <div class="signature-cell">
                            <div class="name">Alexander Peukert</div>
                            <div class="group">Working Group 1</div>
                            <div class="role">Co-Chair</div>
                        </div>
                        <div class="signature-cell">
                            <div class="name">C√©line Castets-Renard</div>
                            <div class="group">Working Group 1</div>
                            <div class="role">Vice-Chair</div>
                        </div>
                    </div>
                    <div class="signature-column">
                        <div class="signature-cell">
                            <div class="name">Matthias Samwald</div>
                            <div class="group">Working Group 2</div>
                            <div class="role">Chair</div>
                        </div>
                        <div class="signature-cell">
                            <div class="name">Marta Ziosi</div>
                            <div class="group">Working Group 2</div>
                            <div class="role">Vice-Chair</div>
                        </div>
                        <div class="signature-cell">
                            <div class="name">Alexander Zacherl</div>
                            <div class="group">Working Group 2</div>
                            <div class="role">Vice-Chair</div>
                        </div>
                    </div>
                    <div class="signature-column">
                        <div class="signature-cell">
                            <div class="name">Yoshua Bengio</div>
                            <div class="group">Working Group 3</div>
                            <div class="role">Chair</div>
                        </div>
                        <div class="signature-cell">
                            <div class="name">Daniel Privitera</div>
                            <div class="group">Working Group 3</div>
                            <div class="role">Vice-Chair</div>
                        </div>
                        <div class="signature-cell">
                            <div class="name">Nitarshan Rajkumar</div>
                            <div class="group">Working Group 3</div>
                            <div class="role">Vice-Chair</div>
                        </div>
                    </div>
                    <div class="signature-column">
                        <div class="signature-cell">
                            <div class="name">Marietje Schaake</div>
                            <div class="group">Working Group 4</div>
                            <div class="role">Chair</div>
                        </div>
                        <div class="signature-cell">
                            <div class="name">Anka Reuel</div>
                            <div class="group">Working Group 4</div>
                            <div class="role">Vice-Chair</div>
                        </div>
                        <div class="signature-cell">
                            <div class="name">Markus Anderljung</div>
                            <div class="group">Working Group 4</div>
                            <div class="role">Vice-Chair</div>
                        </div>
                    </div>
                </div>
            </div>

            <h2>Drafting plan, principles, and assumptions</h2>

            <p>This second draft provides more detailed provisions and concrete examples. At this stage, it still does
                not contain the level of granularity, especially for the KPIs, that we expect to include in the final
                adopted version of the Code. This is because:</p>
            <ul>
                <li>we are still working to achieve broad agreement on the structure and principles of the Code;</li>
                <li>there has been insufficient time to produce a more detailed proposal; and</li>
                <li>we will update the details within the Code on an ongoing basis, to reflect the latest developments
                    and advances in AI.</li>
            </ul>

            <p>The commitments outlined in this Code are organised in a descending hierarchy of Commitments, Measures,
                and KPIs. If any of these elements are absent, particularly KPIs, this is not a definitive decision but
                rather a consequence of the time limitations encountered during the development of this second draft.
                Moreover, this draft does not yet contain a section on how the Code will be reviewed and updated, which
                will be present in later iterations of the draft Code.</p>

            <h3>Below are some high-level principles we follow when drafting the Code:</h3>

            <ol>
                <li><strong>Alignment with EU Principles and Values</strong> ‚Äì Commitments, Measures, and KPIs will be
                    in line with general principles and values of the Union, as enshrined in EU law, including the
                    Charter of Fundamental Rights of the European Union, the Treaty on European Union, and Treaty on the
                    Functioning of the European Union.</li>

                <li><strong>Alignment with AI Act and International Approaches</strong> ‚Äì Commitments, Measures, and
                    KPIs will contribute to a proper application of the AI Act. This includes taking into account
                    international approaches (including standards or metrics developed by AI Safety Institutes, or
                    standard-setting organisations), in accordance with Article 56(1) AI Act.</li>

                <li><strong>Proportionality to Risks</strong> ‚Äì Commitments, Measures, and KPIs should be proportionate
                    to risks, meaning they should be:
                    <ul>
                        <li>suitable to achieve the desired end,</li>
                        <li>necessary to achieve the desired end, and</li>
                        <li>should not impose a burden that is excessive in relation to the end sought to be achieved.
                        </li>
                    </ul>
                    Some concrete applications of proportionality include:
                    <ol type="a">
                        <li>Commitments, Measures, and KPIs should be more stringent for higher risk tiers or uncertain
                            risks of severe harm. The Code can accomplish this by, for example, suggesting multiple KPIs
                            for each Measure related to a severe risk, thereby requiring providers of general-purpose AI
                            models to take action to mitigate that severe risk or to robustly demonstrate an extremely
                            rare likelihood of severe risk eventuating. The Code might also tie risk-mitigating Measures
                            to risk-assessment KPIs, including through the use of "if-then" requirements. For example,
                            if a general-purpose AI model with systemic risk is assessed to have capability X,
                            Signatories commit to putting in place Y risk mitigations, tracked by Z KPIs.</li>

                        <li>Measures and KPIs should be specific. While Commitments may be articulated at a higher level
                            of generality, general-purpose AI model providers should have a clear understanding of how
                            to meet Measures, tracked by KPIs as appropriate. Measures and KPIs should be designed to be
                            effective and robust against misspecification or any attempts of circumvention. The Code
                            strives to accomplish this by, for example, avoiding unnecessary use of proxy terms or
                            metrics. The AI Office will monitor and review Measures and KPIs that may be susceptible to
                            circumvention and other forms of misspecification.</li>

                        <li>Commitments, Measures, and KPIs should differentiate, where applicable, between different
                            types of risks, distribution strategies and deployment contexts of the concerned
                            general-purpose AI model, and other factors that may influence the tiers of risk, and how
                            risks need to be assessed and mitigated. For example, Commitments, Measures, and KPIs
                            assessing and mitigating systemic risks might need to differentiate between intentional and
                            unintentional risks, including instances of misalignment. Additionally, Commitments may need
                            to be adapted to take into account the different tools providers have available to assess
                            and mitigate systemic risk where model weights are freely released.</li>
                    </ol>
                </li>

                <li><strong>Future-Proof</strong> ‚Äì AI technology is changing rapidly. Measures and KPIs should maintain
                    the AI Office's ability to improve its assessment of compliance based on new information. Therefore,
                    the Code shall strive to facilitate the rapid updating of Measures and KPIs, as appropriate. It is
                    important to find a balance between specific requirements and performance indicators on one side,
                    and the flexibility to adapt rapidly to technological and industry developments on the other. The
                    Code can accomplish this by, for example, referencing dynamic sources of information that providers
                    can be expected to monitor and consider in their risk assessment and mitigation.
                    <p>Examples of such sources could include incident databases, consensus standards, up-to-date risk
                        registers, state-of-the-art risk management frameworks, and AI Office guidance. As technology
                        evolves, it may also be necessary to articulate an additional set of Measures and KPIs for
                        specific general-purpose AI models, for example, models used in agentic AI systems.</p>
                </li>

                <li><strong>Proportionality to the size of the general-purpose AI model provider</strong> ‚Äì Measures and
                    KPIs related to the obligations applicable to providers of general-purpose AI models should take due
                    account of the size of the general-purpose AI model provider and allow simplified ways of compliance
                    for small and medium enterprises (SMEs) and start-ups with fewer financial resources than those at
                    the frontier of AI development, where appropriate. KPIs related to the obligations applicable to
                    providers of general-purpose AI models with systemic risk shall also reflect differences in size and
                    capacity of providers, where appropriate.</li>

                <li><strong>Support and growth of the ecosystem for safe, human centric and trustworthy AI</strong> ‚Äì We
                    recognise that the development, adoption, and governance of general-purpose AI models are global
                    issues. Many Commitments in this draft are intended to enable and support cooperation between
                    different stakeholders, for example by sharing general-purpose AI safety infrastructure and best
                    practices amongst model providers, or by encouraging the participation of civil society, academia,
                    third parties, and government organisations in evidence collection. We promote further transparency
                    between stakeholders and increased efforts to share knowledge and cooperate in building a collective
                    and robust evidence base for safe, human centric and trustworthy AI in line with Article 56(1)(3),
                    Recital 1, and Recital 116 AI Act. We also acknowledge the positive impact that open-source models
                    have had on the development of safe, human centric and trustworthy AI.</li>

                <li><strong>Innovation of AI governance and risk management</strong> ‚Äì We recognise that determining the
                    most effective methods for understanding and ensuring the safety of general-purpose AI models
                    remains an evolving challenge. The Code should encourage providers to compete in and advance the
                    state-of-the-art in AI safety governance and related evidence collection methods and practices. When
                    providers can demonstrate equal or superior safety outcomes through alternative approaches that are
                    less burdensome, these innovations should be recognised as improving the state of the art of AI
                    governance and evidence and we should support their wider adoption.</li>
            </ol>

            <p>The current draft is written under the assumption that there will only be a small number of both
                general-purpose AI models with systemic risk and providers thereof. Should that assumption prove wrong,
                future drafts may need to be changed significantly, for instance, by introducing a more detailed tiered
                system of Commitments aiming to focus primarily on those models that provide the largest or most severe
                systemic risks. In particular, we want to highlight that even if modifications of general-purpose AI
                models increase the number of providers in scope, the modifiers' obligations under Articles 53 and 55 AI
                Act should be limited to the extent of their respective modifications, as appropriate. We expect more
                clarifications from the AI Office on these points, as stated in its <a
                    href="https://digital-strategy.ec.europa.eu/en/faqs/general-purpose-ai-models-ai-act-questions-answers">dedicated
                    Q&A</a>.</p>

        </section>

        <!-- Glossary Section -->
        <section class="glossary" id="glossary" role="complementary" aria-label="Glossary">
            <h2>Glossary</h2>
            <dl class="glossary-list">
                <dt id="term-documentation">documentation</dt>
                <dd>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Sed do eiusmod tempor incididunt ut
                    labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation.</dd>

                <dt id="term-code">Code</dt>
                <dd>Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla
                    pariatur. Excepteur sint occaecat cupidatat non proident.</dd>

                <dt id="term-commitments">Commitments</dt>
                <dd>Sed ut perspiciatis unde omnis iste natus error sit voluptatem accusantium doloremque
                    laudantium, totam rem aperiam, eaque ipsa quae ab illo inventore veritatis.</dd>

                <dt id="term-model">model</dt>
                <dd>At vero eos et accusamus et iusto odio dignissimos ducimus qui blanditiis praesentium voluptatum
                    deleniti atque corrupti quos dolores et quas molestias excepturi.</dd>

                <dt id="term-framework">Framework</dt>
                <dd>Nam libero tempore, cum soluta nobis est eligendi optio cumque nihil impedit quo minus id quod
                    maxime placeat facere possimus, omnis voluptas assumenda est.</dd>
            </dl>
        </section>

        <!-- Full Recitals Section -->
        <section class="recitals-full" id="recitals" role="complementary" aria-label="Full recitals">
            <h2>Recitals</h2>
        </section>
    </article>

    <script src="script.js"></script>
</body>

</html>