<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, minimum-scale=1.0">
    <title>EU AI Act Code of Practice</title>
    <link rel="stylesheet" href="https://edwardtufte.github.io/tufte-css/tufte.css" />
    <link rel="stylesheet" href="styles.css" />
</head>

<body>
    <article role="main">
        <!-- Navigation toggle -->
        <button class="nav-toggle" aria-expanded="false" aria-controls="side-nav" aria-label="Toggle navigation menu">
            <span class="nav-icon" aria-hidden="true">&#9776;</span>
            <span class="nav-label">Table of Contents</span>
        </button>

        <!-- Navigation -->
        <nav id="side-nav" class="side-nav" role="navigation" aria-label="Table of contents">
            <div class="nav-controls">
                <div class="theme-control">
                    <label for="theme-select">Theme:</label>
                    <select id="theme-select" aria-label="Select color theme">
                        <option value="system">üñ•Ô∏è System Default</option>
                        <option value="light">‚òÄÔ∏è Light</option>
                        <option value="dark">üåô Dark</option>
                    </select>
                </div>
            </div>
            <div class="keyboard-shortcuts" aria-label="Keyboard shortcuts">
                <h4>Keyboard Shortcuts</h4>
                <ul>
                    <li><kbd>‚Üë</kbd>/<kbd>‚Üì</kbd> Navigate headlines</li>
                    <li><kbd>‚Üê</kbd>/<kbd>‚Üí</kbd> Navigate commitments</li>
                    <li><kbd>1</kbd>To top<kbd>2</kbd>To glossary</li>
                    <li><kbd>3</kbd>To recitals<kbd>4</kbd>Toggle all recitals</li>
                </ul>
            </div>
            <div id="nav-content" class="nav-content">
            </div>
        </nav>

        <h1>EU AI Act Code of Practice</h1>

        <section class="main-content" aria-label="Main content">
            <p class="subtitle">Second Draft - 19/12/2024</p>

            <div class="disclaimer-box" role="complementary" aria-label="Important disclaimer">
                <h4>IMPORTANT DISCLAIMER & HOW TO CONTRIBUTE</h4>
                <p>This is a free, best-effort service to make the EU AI Act Code of Practice more accessible to
                    stakeholders. While we strive
                    for accuracy, this is NOT an official source.</p>

                <p><strong>Please always refer to the <a
                            href="https://digital-strategy.ec.europa.eu/en/library/second-draft-general-purpose-ai-code-practice-published-written-independent-experts">official
                            PDF document</a> for the authoritative version.</strong> Any discrepancies between this
                    version and the official document should be considered errors on our part.</p>

                <p>Help us improve:</p>
                <ul>
                    <li>Report errors via email: <a
                            href="mailto:alexander.zacherl@googlemail.com">alexander.zacherl@googlemail.com</a></li>
                    <li>Submit a pull request on <a href="https://github.com/yourusername/cop-reader">GitHub</a></li>
                </ul>
            </div>

            <h2>Opening statement by the Chairs and Vice-Chairs</h2>

            <p>As the Chairs and Vice-Chairs of the four Working Groups, we hereby present the second draft of the
                General-Purpose AI Code of Practice under the AI Act (the "Code"). Participants in the Working Groups
                and observers of the Code of Practice Plenary are welcome to submit written feedback on this draft by 15
                January 2025 via a dedicated survey shared with them.</p>

            <p>We would like to emphasise that the second draft remains a work-in-progress. Given the short timeframe
                between receiving feedback on the first draft and publishing this second draft, we have focused
                primarily on providing clarifications, adding essential details, and refining our approach to
                proportionality.</p>

            <p>Incorporation of specific feedback at this stage does not guarantee its inclusion in the final Code ‚Äì we
                will have more time to carefully discuss and evaluate various Commitments and Measures before the third
                draft, and significant updates will likely occur. Conversely, where certain elements remain unchanged in
                this draft, this does not indicate permanence ‚Äì we simply may not have addressed these aspects yet.</p>

            <p>This second draft of the Code addresses key considerations for providers of general-purpose AI models and
                providers of general-purpose AI models with systemic risk when complying with Chapter V of the AI Act,
                through four Working Groups working in close collaboration:</p>
            <ul>
                <li>Working Group 1: Transparency and copyright-related rules</li>
                <li>Working Group 2: Risk assessment for systemic risk</li>
                <li>Working Group 3: Technical risk mitigation for systemic risk</li>
                <li>Working Group 4: Governance risk mitigation for systemic risk</li>
            </ul>

            <p>Working Group 1 Transparency applies to all general-purpose AI models, except for those that are released
                under a free and open-source licence satisfying the conditions specified in Article 53(2) AI Act and not
                classified as general-purpose AI models with systemic risk. Working Group 1 Copyright applies to all
                general-purpose AI models. Working Groups 2, 3, and 4, along with the corresponding Section III only
                apply to providers of general-purpose AI models classified as general-purpose AI models with systemic
                risk based on Article 51 AI Act.</p>

            <p>Following a thorough review of the feedback received by stakeholders on the first draft, we have refined
                Commitments and Measures and expanded the Code's provisions while maintaining its Objectives. We present
                this second draft as a foundation for further development. The next draft will draw on your feedback
                provided via the EU survey, in provider workshops, and in Working Group meetings. Thus far, we have
                found your feedback extremely helpful, resulting in substantial changes. We therefore encourage
                stakeholders to continue providing comprehensive feedback on all aspects of the Code, including both new
                and unchanged elements. Your feedback will help shape the final version of the Code, which will play a
                crucial role in guiding the future of general-purpose AI model development and deployment.</p>

            <p>We have once again included a high-level drafting plan which outlines our guiding principles for the
                Code, and the assumptions it is based on. While we continue to engage in thorough deliberations
                regarding specific Commitments, Measures and Key Performance Indicators (KPIs), we hope the drafting
                plan provides stakeholders clarity on the potential form and content of the final Code.</p>

            <p>Note that the exemplary KPIs included in this version of the Code are preliminary, and subject to review
                and revision. For example, while some are quantitative, others are more qualitative. Thus, we strongly
                encourage, and welcome, feedback on the KPIs.</p>

            <p>The AI Act came into force on 1 August 2024, stating that the final version of the Code should be ready
                by 2 May 2025. The second draft builds upon previous work while aiming to provide a "future-proof" Code,
                appropriate for the next generation of models which will be developed and released in 2025 and
                thereafter.</p>

            <p>In formulating this second draft, we have been principally guided by the provisions in the AI Act as to
                matters within the scope of the Code. Accordingly, unless the context and definition contained within
                the Code indicates otherwise, the terms used in the Code refer to identical terms from the AI Act. We
                have not included exhaustive references to provisions in the AI Act in this second draft but expect to
                do so in future iterations.</p>

            <p>Like the first draft, this document is the result of a collaborative effort involving hundreds of
                participants from across industry, academia, and civil society. It has been informed by feedback
                received in response to the first draft, which has been insightful and instructive in our drafting
                process. We continue to be informed by the evolving literature on AI governance, international
                approaches (as specified in Article 56(1) AI Act), Union law codes of practice (such as the Code of
                Practice on Disinformation), industry best practice, and the expertise and experience of providers and
                Working Group members.</p>

            <h3>Key features of the development process of the Code include:</h3>
            <ul>
                <li>Drafted by Chairs and Vice-Chairs who were selected by the AI Office for their expertise,
                    experience, independence (including absence of financial interests), and to ensure gender and
                    geographic diversity.</li>
                <li>A multi-stakeholder consultation which closed in September and received 427 submissions</li>
                <li>A multi-stakeholder survey on the first draft of the Code which received 354 submissions, with more
                    surveys to come</li>
                <li>Organisational support and legal advice from the AI Office</li>
                <li>Provider workshops led by Chairs and Vice-Chairs</li>
                <li>Four specialised Working Group meetings led by Chairs and Vice-Chairs</li>
                <li>Meetings with representatives from EU Member States in the AI Board and from the European Parliament
                </li>
            </ul>

            <p>Additional time for consultation and deliberation ‚Äì both externally and internally ‚Äì will be needed to
                refine and improve the current draft. As a group of independent Chairs and Vice-Chairs, we strive to
                make this process as transparent and accessible to stakeholders as possible, aiming to share our work
                and our thinking as early as possible, while taking sufficient time to coordinate and discuss key
                questions within Working Groups. We count on your continued engaged collaboration and constructive
                criticism.</p>

            <p>Finally, we wish to highlight that, at this stage in the drafting process, one of our central priorities
                has been to clearly communicate our motivations and reasoning regarding the issues we are addressing.
                Many of these issues are nuanced and complex, and we aim to convey them transparently through the draft
                text of the Code. Further, we used our time mainly to refine Commitments, Measures, and KPIs, rather
                than ensuring all the parts of the Code seamlessly fit together and are easy to understand. We will work
                to improve these aspects in subsequent iterations to strengthen the Code.</p>

            <p>We welcome written feedback by the Code of Practice Plenary participants and observers by 15 January
                2025, via a dedicated survey shared with them.</p>

            <p>Thank you for your support!</p>

            <div class="signatures">
                <div class="signatures-grid">
                    <div class="signature-column">
                        <div class="signature-cell">
                            <div class="name">Nuria Oliver</div>
                            <div class="group">Working Group 1</div>
                            <div class="role">Co-Chair</div>
                        </div>
                        <div class="signature-cell">
                            <div class="name">Rishi Bommasani</div>
                            <div class="group">Working Group 1</div>
                            <div class="role">Vice-Chair</div>
                        </div>
                    </div>
                    <div class="signature-column">
                        <div class="signature-cell">
                            <div class="name">Alexander Peukert</div>
                            <div class="group">Working Group 1</div>
                            <div class="role">Co-Chair</div>
                        </div>
                        <div class="signature-cell">
                            <div class="name">C√©line Castets-Renard</div>
                            <div class="group">Working Group 1</div>
                            <div class="role">Vice-Chair</div>
                        </div>
                    </div>
                    <div class="signature-column">
                        <div class="signature-cell">
                            <div class="name">Matthias Samwald</div>
                            <div class="group">Working Group 2</div>
                            <div class="role">Chair</div>
                        </div>
                        <div class="signature-cell">
                            <div class="name">Marta Ziosi</div>
                            <div class="group">Working Group 2</div>
                            <div class="role">Vice-Chair</div>
                        </div>
                        <div class="signature-cell">
                            <div class="name">Alexander Zacherl</div>
                            <div class="group">Working Group 2</div>
                            <div class="role">Vice-Chair</div>
                        </div>
                    </div>
                    <div class="signature-column">
                        <div class="signature-cell">
                            <div class="name">Yoshua Bengio</div>
                            <div class="group">Working Group 3</div>
                            <div class="role">Chair</div>
                        </div>
                        <div class="signature-cell">
                            <div class="name">Daniel Privitera</div>
                            <div class="group">Working Group 3</div>
                            <div class="role">Vice-Chair</div>
                        </div>
                        <div class="signature-cell">
                            <div class="name">Nitarshan Rajkumar</div>
                            <div class="group">Working Group 3</div>
                            <div class="role">Vice-Chair</div>
                        </div>
                    </div>
                    <div class="signature-column">
                        <div class="signature-cell">
                            <div class="name">Marietje Schaake</div>
                            <div class="group">Working Group 4</div>
                            <div class="role">Chair</div>
                        </div>
                        <div class="signature-cell">
                            <div class="name">Anka Reuel</div>
                            <div class="group">Working Group 4</div>
                            <div class="role">Vice-Chair</div>
                        </div>
                        <div class="signature-cell">
                            <div class="name">Markus Anderljung</div>
                            <div class="group">Working Group 4</div>
                            <div class="role">Vice-Chair</div>
                        </div>
                    </div>
                </div>
            </div>

            <h2>Drafting plan, principles, and assumptions</h2>

            <p>This second draft provides more detailed provisions and concrete examples. At this stage, it still does
                not contain the level of granularity, especially for the KPIs, that we expect to include in the final
                adopted version of the Code. This is because:</p>
            <ul>
                <li>we are still working to achieve broad agreement on the structure and principles of the Code;</li>
                <li>there has been insufficient time to produce a more detailed proposal; and</li>
                <li>we will update the details within the Code on an ongoing basis, to reflect the latest developments
                    and advances in AI.</li>
            </ul>

            <p>The commitments outlined in this Code are organised in a descending hierarchy of Commitments, Measures,
                and KPIs. If any of these elements are absent, particularly KPIs, this is not a definitive decision but
                rather a consequence of the time limitations encountered during the development of this second draft.
                Moreover, this draft does not yet contain a section on how the Code will be reviewed and updated, which
                will be present in later iterations of the draft Code.</p>

            <h3>Below are some high-level principles we follow when drafting the Code:</h3>

            <ol>
                <li><strong>Alignment with EU Principles and Values</strong> ‚Äì Commitments, Measures, and KPIs will be
                    in line with general principles and values of the Union, as enshrined in EU law, including the
                    Charter of Fundamental Rights of the European Union, the Treaty on European Union, and Treaty on the
                    Functioning of the European Union.</li>

                <li><strong>Alignment with AI Act and International Approaches</strong> ‚Äì Commitments, Measures, and
                    KPIs will contribute to a proper application of the AI Act. This includes taking into account
                    international approaches (including standards or metrics developed by AI Safety Institutes, or
                    standard-setting organisations), in accordance with Article 56(1) AI Act.</li>

                <li><strong>Proportionality to Risks</strong> ‚Äì Commitments, Measures, and KPIs should be proportionate
                    to risks, meaning they should be:
                    <ul>
                        <li>suitable to achieve the desired end,</li>
                        <li>necessary to achieve the desired end, and</li>
                        <li>should not impose a burden that is excessive in relation to the end sought to be achieved.
                        </li>
                    </ul>
                    Some concrete applications of proportionality include:
                    <ol type="a">
                        <li>Commitments, Measures, and KPIs should be more stringent for higher risk tiers or uncertain
                            risks of severe harm. The Code can accomplish this by, for example, suggesting multiple KPIs
                            for each Measure related to a severe risk, thereby requiring providers of general-purpose AI
                            models to take action to mitigate that severe risk or to robustly demonstrate an extremely
                            rare likelihood of severe risk eventuating. The Code might also tie risk-mitigating Measures
                            to risk-assessment KPIs, including through the use of "if-then" requirements. For example,
                            if a general-purpose AI model with systemic risk is assessed to have capability X,
                            Signatories commit to putting in place Y risk mitigations, tracked by Z KPIs.</li>

                        <li>Measures and KPIs should be specific. While Commitments may be articulated at a higher level
                            of generality, general-purpose AI model providers should have a clear understanding of how
                            to meet Measures, tracked by KPIs as appropriate. Measures and KPIs should be designed to be
                            effective and robust against misspecification or any attempts of circumvention. The Code
                            strives to accomplish this by, for example, avoiding unnecessary use of proxy terms or
                            metrics. The AI Office will monitor and review Measures and KPIs that may be susceptible to
                            circumvention and other forms of misspecification.</li>

                        <li>Commitments, Measures, and KPIs should differentiate, where applicable, between different
                            types of risks, distribution strategies and deployment contexts of the concerned
                            general-purpose AI model, and other factors that may influence the tiers of risk, and how
                            risks need to be assessed and mitigated. For example, Commitments, Measures, and KPIs
                            assessing and mitigating systemic risks might need to differentiate between intentional and
                            unintentional risks, including instances of misalignment. Additionally, Commitments may need
                            to be adapted to take into account the different tools providers have available to assess
                            and mitigate systemic risk where model weights are freely released.</li>
                    </ol>
                </li>

                <li><strong>Future-Proof</strong> ‚Äì AI technology is changing rapidly. Measures and KPIs should maintain
                    the AI Office's ability to improve its assessment of compliance based on new information. Therefore,
                    the Code shall strive to facilitate the rapid updating of Measures and KPIs, as appropriate. It is
                    important to find a balance between specific requirements and performance indicators on one side,
                    and the flexibility to adapt rapidly to technological and industry developments on the other. The
                    Code can accomplish this by, for example, referencing dynamic sources of information that providers
                    can be expected to monitor and consider in their risk assessment and mitigation.
                    <p>Examples of such sources could include incident databases, consensus standards, up-to-date risk
                        registers, state-of-the-art risk management frameworks, and AI Office guidance. As technology
                        evolves, it may also be necessary to articulate an additional set of Measures and KPIs for
                        specific general-purpose AI models, for example, models used in agentic AI systems.</p>
                </li>

                <li><strong>Proportionality to the size of the general-purpose AI model provider</strong> ‚Äì Measures and
                    KPIs related to the obligations applicable to providers of general-purpose AI models should take due
                    account of the size of the general-purpose AI model provider and allow simplified ways of compliance
                    for small and medium enterprises (SMEs) and start-ups with fewer financial resources than those at
                    the frontier of AI development, where appropriate. KPIs related to the obligations applicable to
                    providers of general-purpose AI models with systemic risk shall also reflect differences in size and
                    capacity of providers, where appropriate.</li>

                <li><strong>Support and growth of the ecosystem for safe, human centric and trustworthy AI</strong> ‚Äì We
                    recognise that the development, adoption, and governance of general-purpose AI models are global
                    issues. Many Commitments in this draft are intended to enable and support cooperation between
                    different stakeholders, for example by sharing general-purpose AI safety infrastructure and best
                    practices amongst model providers, or by encouraging the participation of civil society, academia,
                    third parties, and government organisations in evidence collection. We promote further transparency
                    between stakeholders and increased efforts to share knowledge and cooperate in building a collective
                    and robust evidence base for safe, human centric and trustworthy AI in line with Article 56(1)(3),
                    Recital 1, and Recital 116 AI Act. We also acknowledge the positive impact that open-source models
                    have had on the development of safe, human centric and trustworthy AI.</li>

                <li><strong>Innovation of AI governance and risk management</strong> ‚Äì We recognise that determining the
                    most effective methods for understanding and ensuring the safety of general-purpose AI models
                    remains an evolving challenge. The Code should encourage providers to compete in and advance the
                    state-of-the-art in AI safety governance and related evidence collection methods and practices. When
                    providers can demonstrate equal or superior safety outcomes through alternative approaches that are
                    less burdensome, these innovations should be recognised as improving the state of the art of AI
                    governance and evidence and we should support their wider adoption.</li>
            </ol>

            <p>The current draft is written under the assumption that there will only be a small number of both
                general-purpose AI models with systemic risk and providers thereof. Should that assumption prove wrong,
                future drafts may need to be changed significantly, for instance, by introducing a more detailed tiered
                system of Commitments aiming to focus primarily on those models that provide the largest or most severe
                systemic risks. In particular, we want to highlight that even if modifications of general-purpose AI
                models increase the number of providers in scope, the modifiers' obligations under Articles 53 and 55 AI
                Act should be limited to the extent of their respective modifications, as appropriate. We expect more
                clarifications from the AI Office on these points, as stated in its <a
                    href="https://digital-strategy.ec.europa.eu/en/faqs/general-purpose-ai-models-ai-act-questions-answers">dedicated
                    Q&A</a>.</p>

            <h2>I. PREAMBLE</h2>

            <h3>Whereas:</h3>
            <ol type="a">
                <li>The Signatories of this Code of Practice (Code) recognise the importance of improving the
                    functioning of the internal market, of creating a level playing field for the regulation of
                    human-centric and trustworthy artificial intelligence (AI), while ensuring a high level of
                    protection of health, safety, fundamental rights enshrined in the Charter, including democracy, the
                    rule of law and environmental protection, against harmful effects of AI in the Union and supporting
                    innovation as emphasised in Article 1(1) AI Act. The Code shall be interpreted in this context.</li>

                <li>Whenever the Code refers to providers of general-purpose AI models it shall encompass providers of
                    general-purpose AI models with systemic risk, too. Whenever the Code refers to providers of
                    general-purpose AI models with systemic risk it shall not encompass providers of other
                    general-purpose AI models.</li>

                <li>The Signatories recognise that the Code serves as a guiding document for providers of
                    general-purpose AI models and general-purpose AI models with systemic risk in demonstrating
                    compliance with the AI Act, while recognising that adherence to this Code does not constitute
                    conclusive evidence of compliance with the AI Act.</li>

                <li>The Signatories recognise the importance of reporting their implementation of the Code and its
                    outcomes to facilitate the regular monitoring and evaluation of the Code's adequacy by the AI Office
                    and the Board (Article 56(5) AI Act).</li>

                <li>The Signatories recognise that the Code shall be subject to regular review by the AI Office. The AI
                    Office may encourage and facilitate updates of the Code to reflect advances in AI technology,
                    societal changes, and emerging systemic risks (Article 56(6) AI Act).</li>

                <li>The Signatories recognise that the Code may serve as a bridge until the adoption of harmonised EU
                    standards for general-purpose AI models. Updates may be needed to facilitate a gradual transition
                    towards future standards.</li>

                <li>The Signatories recognise that the absence of specific Commitments, Measures, and Key Performance
                    Indicators (KPIs) within this Code does not absolve providers of general-purpose AI models with
                    systemic risk from their responsibility to address and mitigate potential systemic risks as they
                    emerge.</li>

                <li>The Signatories recognise the importance of working in partnership with the AI Office to foster
                    collaboration between providers of general-purpose AI models, researchers, and regulatory bodies to
                    address emerging challenges and opportunities in the AI landscape.</li>
            </ol>

            <h3>The Objectives of the Code are as follows:</h3>
            <ol type="I">
                <li>Providers of general-purpose AI models can effectively comply with their obligations under the AI
                    Act. The Code of Practice should clarify to providers how to demonstrate compliance. The Code should
                    also enable the AI Office to assess the compliance of providers who choose to rely on the Code to
                    demonstrate compliance, in accordance with Articles 53(4) and 55(2) AI Act. This can include
                    allowing sufficient visibility into trends in the development and deployment of general-purpose AI
                    models, particularly of the most advanced models.</li>

                <li>Providers of general-purpose AI models can effectively ensure a good understanding of
                    general-purpose AI models along the AI value chain, both to enable the integration of such models
                    into downstream products and to fulfil subsequent obligations under the AI Act or other regulations
                    (see Article 53 and Recital 101 AI Act).</li>

                <li>Providers of general-purpose AI models can effectively comply with Union law on copyright and
                    related rights (see Article 53 and Recital 106 AI Act).</li>

                <li>Providers of general-purpose AI models with systemic risk can effectively continuously assess and
                    mitigate possible systemic risks at the Union level, including their sources, that may stem from the
                    development, the placing on the market, or the use of general-purpose AI models with systemic risk
                    (see Article 55 and Recital 114 AI Act).</li>
            </ol>

            <h2>II. COMMITMENTS BY PROVIDERS OF GENERAL-PURPOSE AI MODELS</h2>

            <h3>Whereas:</h3>
            <ol type="a">
                <li>The Signatories recognise the particular role and responsibility of providers of general-purpose AI
                    models along the AI value chain, as the models they provide may form the basis for a range of
                    downstream systems, often provided by downstream providers that need significant understanding of
                    the models and their capabilities, both to enable the integration of such models into their products
                    and to fulfil their obligations under the AI Act (see Recital 101 AI Act).</li>

                <li>The Signatories recognise that in the case of a modification or fine-tuning of a model, the
                    obligations for providers should be limited to that modification or fine-tuning to safeguard
                    proportionality (see Recital 109 AI Act).</li>

                <li>The AI Act and the Code are without prejudice to the rules laid down by Union and national law, and
                    the Code shall be interpreted in particular in accordance with Union copyright law. Directive (EU)
                    2019/790 introduced exceptions and limitations allowing reproductions and extractions of works or
                    other subject matter, for the purpose of text and data mining, under certain conditions.
                    Under these rules, rightsholders may choose to reserve their rights over their works or other
                    subject matter to prevent text and data mining, unless this is done for the purposes of scientific
                    research.
                    Where reservations of rights have been expressed in an appropriate manner, providers of
                    general-purpose AI models need to obtain an authorisation from rightsholders if they want to carry
                    out text and data mining over such works (see Recital 105 AI Act).</li>
            </ol>

            <p>Therefore, the Signatories of this Code commit to the following:</p>

            <div class="legal-text">
                <h4>TRANSPARENCY LEGAL TEXT</h4>
                <p><strong>Article 53(1), point (a) AI Act:</strong> "Providers of general-purpose AI models shall draw
                    up and keep up-to-date the technical documentation of the model, including its training and testing
                    process and the results of its evaluation, which shall contain, at a minimum, the information set
                    out in Annex XI for the purpose of providing it, upon request, to the AI Office and the national
                    competent authorities;"</p>

                <p><strong>Article 53(1), point (b) AI Act:</strong> "Providers of general-purpose AI models shall draw
                    up, keep up-to-date and make available information and documentation to providers of AI systems who
                    intend to integrate the general-purpose AI model into their AI systems. Without prejudice to the
                    need to observe and protect intellectual property rights and confidential business information or
                    trade secrets in accordance with Union and national law, the information and documentation shall:
                    <br>(i) enable providers of AI systems to have a good understanding of the capabilities and
                    limitations of the general-purpose AI model and to comply with their obligations pursuant to this
                    Regulation; and
                    <br>(ii) contain, at a minimum, the elements set out in Annex XII;"
                </p>

                <p><strong>Article 53(2) AI Act:</strong> "The obligations set out in paragraph 1, points (a) and (b),
                    shall not apply to providers of AI models that are released under a free and open-source licence
                    that allows for the access, usage, modification, and distribution of the model, and whose
                    parameters, including the weights, the information on the model architecture, and the information on
                    model usage, are made publicly available. This exception shall not apply to general-purpose AI
                    models with systemic risks."</p>

                <p><strong>Article 53(7) AI Act:</strong> "Any information or documentation obtained pursuant to this
                    Article, including trade secrets, shall be treated in accordance with the confidentiality
                    obligations set out in Article 78."</p>
            </div>

            <h3>Commitment 1. Documentation</h3>
            <p>In order to fulfil the obligations in Article 53(1), points (a) and (b) AI Act, Signatories commit to the
                Measures specified below. These Measures do not apply to providers of open-source AI models satisfying
                the conditions specified in Article 53(2) AI Act, unless the models are general-purpose AI models with
                systemic risk. For Signatories who are providers of general-purpose AI models with systemic risk,
                Measure 20.2 in this Code covers the additional documentation required by Article 53(1), point (a) AI
                Act (more specifically the documentation listed in Annex XI Section 2 AI Act).</p>

            <h4>Measure 1.1. Drawing up, keeping up-to-date, and providing the relevant information</h4>
            <p>Signatories commit to drawing up and providing the information listed in Table 1 below to the AI Office
                and national competent authorities upon request, and/or to downstream providers, with the disclosed
                information safeguarded by the trade secrets and confidentiality protections provided by Article 53(1),
                point (b), and (7) AI Act.</p>

            <p>Signatories are encouraged to consider whether the documented information can be disclosed, in whole or
                in part, to the public to promote public transparency. Some of this information may also be requested in
                summarised form as part of the public summary for training content that providers must make publicly
                available under Article 53(1), point (d) AI Act to be determined in a template to be provided by the AI
                Office.</p>

            <p>Signatories commit to ensuring that the documented information is reviewed and updated when necessary,
                including to reflect any changes to the general-purpose AI model.</p>

            <h4>Measure 1.2. Ensuring quality, integrity, and security of information</h4>
            <p>Signatories commit to ensuring that the documented information is controlled for quality and integrity,
                retained as evidence of compliance with obligations of the AI Act, and protected from unintended
                alterations.</p>

            <p>In the context of drawing-up, updating, and controlling the quality and security of the information and
                records, Signatories are encouraged to follow the established protocols and technical standards.</p>

            <div class="info-box">
                <h4>Appendix: Essential elements of an Acceptable Use Policy</h4>
                <p>An Acceptable Use Policy (AUP) is defined as guidelines to users on what is and is not considered
                    acceptable use.</p>

                <p>Essential elements of an AUP are:</p>
                <ul>
                    <li>A purpose statement explaining why the AUP exists;</li>
                    <li>The scope defining who the policy applies to and what resources it covers;</li>
                    <li>Main intended uses and users;</li>
                    <li>Acceptable uses, listing activities and tasks that are allowed, including high-risk AI uses
                        (within the meaning of Article 6 AI Act in conjunction with Annex I and III AI Act), if any,
                        that the model is intended to be integrated into;</li>
                    <li>Unacceptable uses, detailing forbidden actions (beyond those prohibited by Article 5 AI Act);
                    </li>
                    <li>Security measures containing a description of the security protocols that the users of the model
                        must follow;</li>
                    <li>If any monitoring of the use of their model is performed by the provider, an explanation of how
                        the monitoring occurs and its impact on users' privacy and confidentiality of users' business
                        information;</li>
                    <li>Warning processes and criteria for suspension or withdrawal of user privileges for not adhering
                        to the AUP;</li>
                    <li>Criteria for terminating user accounts and reference to applicable law and regulations for
                        enforcement;</li>
                    <li>Acknowledgement from users that they have read, understood, and agreed to comply with the AUP.
                    </li>
                </ul>
            </div>

        </section>

        <!-- Glossary Section -->
        <section class="glossary" id="glossary" role="complementary" aria-label="Glossary">
            <h2>Glossary</h2>
            <dl class="glossary-list">
                <dt id="term-documentation">documentation</dt>
                <dd>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Sed do eiusmod tempor incididunt ut
                    labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation.</dd>

                <dt id="term-code">Code</dt>
                <dd>Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla
                    pariatur. Excepteur sint occaecat cupidatat non proident.</dd>

                <dt id="term-commitments">Commitments</dt>
                <dd>Sed ut perspiciatis unde omnis iste natus error sit voluptatem accusantium doloremque
                    laudantium, totam rem aperiam, eaque ipsa quae ab illo inventore veritatis.</dd>

                <dt id="term-model">model</dt>
                <dd>At vero eos et accusamus et iusto odio dignissimos ducimus qui blanditiis praesentium voluptatum
                    deleniti atque corrupti quos dolores et quas molestias excepturi.</dd>

                <dt id="term-framework">Framework</dt>
                <dd>Nam libero tempore, cum soluta nobis est eligendi optio cumque nihil impedit quo minus id quod
                    maxime placeat facere possimus, omnis voluptas assumenda est.</dd>
            </dl>
        </section>

        <!-- Full Recitals Section -->
        <section class="recitals-full" id="recitals" role="complementary" aria-label="Full recitals">
            <h2>Recitals</h2>
        </section>
    </article>

    <script src="script.js"></script>
</body>

</html>